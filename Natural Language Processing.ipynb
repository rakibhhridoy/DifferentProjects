{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "!pip install tensorflow==2.0.0-alpha0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    'I love you Murshida',\n",
    "    'I love you a lot',\n",
    "    'You also love me',\n",
    "    'So both us love each other, Murshida!'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=100)\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'love': 1, 'you': 2, 'i': 3, 'murshida': 4, 'a': 5, 'lot': 6, 'also': 7, 'me': 8, 'so': 9, 'both': 10, 'us': 11, 'each': 12, 'other': 13}\n"
     ]
    }
   ],
   "source": [
    "print(word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3, 1, 2, 4], [3, 1, 2, 5, 6], [2, 7, 1, 8], [9, 10, 11, 1, 12, 13, 4]]\n"
     ]
    }
   ],
   "source": [
    "print(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = [\n",
    "    'i really love you!',\n",
    "    'my heart beats for you'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_seq = tokenizer.texts_to_sequences(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3, 1, 2], [2]]\n"
     ]
    }
   ],
   "source": [
    "print(test_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### oov_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<OOV>': 1, 'love': 2, 'you': 3, 'i': 4, 'murshida': 5, 'a': 6, 'lot': 7, 'also': 8, 'me': 9, 'so': 10, 'both': 11, 'us': 12, 'each': 13, 'other': 14}\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=100, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "print(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "test_data = [\n",
    "    'i really love you!',\n",
    "    'my heart beats for you'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4, 1, 2, 3], [1, 1, 1, 1, 3]]\n"
     ]
    }
   ],
   "source": [
    "test_seq = tokenizer.texts_to_sequences(test_data)\n",
    "print(test_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded = pad_sequences(sequences, padding= 'post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4  2  3  5  0  0  0]\n",
      " [ 4  2  3  6  7  0  0]\n",
      " [ 3  8  2  9  0  0  0]\n",
      " [10 11 12  2 13 14  5]]\n"
     ]
    }
   ],
   "source": [
    "print(padded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Trauncating- removing word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded = pad_sequences(sequences, padding='post', maxlen= 5, truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4  2  3  5  0]\n",
      " [ 4  2  3  6  7]\n",
      " [ 3  8  2  9  0]\n",
      " [10 11 12  2 13]]\n"
     ]
    }
   ],
   "source": [
    "print(padded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sarcasm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for line in open('Sarcasm/Sarcasm_Headlines_Dataset.json', 'r'):\n",
    "    data.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "for dicts in data:\n",
    "    for item in dicts.values():\n",
    "        items.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "i = 1\n",
    "while i <= len(items):\n",
    "    sentences.append(items[i])\n",
    "    i += 3\n",
    "    \n",
    "    \n",
    "    \n",
    "urls = []\n",
    "i = 0\n",
    "while i< len(items):\n",
    "    urls.append(items[i])\n",
    "    i += 3\n",
    "\n",
    "    \n",
    "labels = []\n",
    "i = 2\n",
    "while i<= len(items):\n",
    "    labels.append(items[i])\n",
    "    i += 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26709"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26709"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26709"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 1, 1, 0, 0, 0, 0, 1, 0]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"former versace store clerk sues over secret 'black code' for minority shoppers\",\n",
       " \"the 'roseanne' revival catches up to our thorny political mood, for better and worse\",\n",
       " \"mom starting to fear son's web series closest thing she will have to grandchild\",\n",
       " 'boehner just wants wife to listen, not come up with alternative debt-reduction ideas',\n",
       " 'j.k. rowling wishes snape happy birthday in the most magical way',\n",
       " \"advancing the world's women\",\n",
       " 'the fascinating case for eating lab-grown meat',\n",
       " 'this ceo will send your kids to school, if you work for his company',\n",
       " 'top snake handler leaves sinking huckabee campaign',\n",
       " \"friday's morning email: inside trump's presser for the ages\"]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.huffingtonpost.com/entry/versace-black-code_us_5861fbefe4b0de3a08f600d5',\n",
       " 'https://www.huffingtonpost.com/entry/roseanne-revival-review_us_5ab3a497e4b054d118e04365',\n",
       " 'https://local.theonion.com/mom-starting-to-fear-son-s-web-series-closest-thing-she-1819576697',\n",
       " 'https://politics.theonion.com/boehner-just-wants-wife-to-listen-not-come-up-with-alt-1819574302',\n",
       " 'https://www.huffingtonpost.com/entry/jk-rowling-wishes-snape-happy-birthday_us_569117c4e4b0cad15e64fdcb',\n",
       " 'https://www.huffingtonpost.com/entry/advancing-the-worlds-women_b_6810038.html',\n",
       " 'https://www.huffingtonpost.com/entry/how-meat-is-grown-in-a-lab_us_561d1189e4b0c5a1ce607e86',\n",
       " 'https://www.huffingtonpost.com/entry/boxed-college-tuition-ben_n_7445644.html',\n",
       " 'https://politics.theonion.com/top-snake-handler-leaves-sinking-huckabee-campaign-1819578231',\n",
       " 'https://www.huffingtonpost.com/entry/fridays-morning-email-inside-trumps-presser-for-the-ages_us_58a6e33ee4b07602ad53a315']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenier = Tokenizer(oov_token= '<OOV>')\n",
    "\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "padded = pad_sequences(sequences, padding = 'post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mom starting to fear son's web series closest thing she will have to grandchild\n",
      "[ 1  1  2  1  1  1  1  1  1  1 39 46  2  1  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "(26709, 40)\n"
     ]
    }
   ],
   "source": [
    "print(sentences[2])\n",
    "print(padded[2])\n",
    "print(padded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_size = 20000\n",
    "vocab_size = 1000\n",
    "max_length = 16\n",
    "padding_type = 'post'\n",
    "embedding_dim = 32\n",
    "trunc_type = 'post'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_sentences = sentences[0:training_size]\n",
    "training_labels = labels[0:training_size]\n",
    "\n",
    "testing_sentences = sentences[training_size:]\n",
    "testing_labels = labels[training_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words= 100000, oov_token= '<oov>')\n",
    "\n",
    "tokenizer.fit_on_texts(training_sentences)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "training_sequence = tokenizer.texts_to_sequences(training_sentences)\n",
    "training_padded = pad_sequences(training_sequence, maxlen= max_length , truncating=trunc_type, padding= padding_type)\n",
    "\n",
    "testing_sequence = tokenizer.texts_to_sequences(testing_sentences)\n",
    "testing_padded = pad_sequences(testing_sequence, maxlen= max_length, truncating= trunc_type, padding= padding_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras as tk\n",
    "from tensorflow.keras import layers as tkl\n",
    "from tensorflow.keras.layers import Dense as tkld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tk.Sequential([\n",
    "    tkl.Embedding(vocab_size, embedding_dim, input_length= max_length),\n",
    "    tkl.GlobalAveragePooling1D(),\n",
    "    tkld(24, activation= 'relu'),\n",
    "    tkld(1, activation= 'sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 16, 32)            32000     \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 24)                792       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 25        \n",
      "=================================================================\n",
      "Total params: 32,817\n",
      "Trainable params: 32,817\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:absl:Failed to construct dataset imdb_reviews\n"
     ]
    },
    {
     "ename": "ParseError",
     "evalue": "mismatched tag: line 8, column 2 (<string>)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[1;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[0;32m\"C:\\Users\\User\\anaconda\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\"\u001b[0m, line \u001b[0;32m3331\u001b[0m, in \u001b[0;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \u001b[0;32m\"<ipython-input-39-3f9260b54fda>\"\u001b[0m, line \u001b[0;32m1\u001b[0m, in \u001b[0;35m<module>\u001b[0m\n    imdb, info = tfds.load('imdb_reviews/subwords8k', with_info= True, as_supervised= True)\n",
      "  File \u001b[0;32m\"C:\\Users\\User\\anaconda\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow_datasets\\core\\api_utils.py\"\u001b[0m, line \u001b[0;32m52\u001b[0m, in \u001b[0;35mdisallow_positional_args_dec\u001b[0m\n    return fn(*args, **kwargs)\n",
      "  File \u001b[0;32m\"C:\\Users\\User\\anaconda\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow_datasets\\core\\registered.py\"\u001b[0m, line \u001b[0;32m297\u001b[0m, in \u001b[0;35mload\u001b[0m\n    dbuilder = builder(name, data_dir=data_dir, **builder_kwargs)\n",
      "  File \u001b[0;32m\"C:\\Users\\User\\anaconda\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow_datasets\\core\\registered.py\"\u001b[0m, line \u001b[0;32m169\u001b[0m, in \u001b[0;35mbuilder\u001b[0m\n    return _DATASET_REGISTRY[name](**builder_kwargs)\n",
      "  File \u001b[0;32m\"C:\\Users\\User\\anaconda\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow_datasets\\core\\api_utils.py\"\u001b[0m, line \u001b[0;32m52\u001b[0m, in \u001b[0;35mdisallow_positional_args_dec\u001b[0m\n    return fn(*args, **kwargs)\n",
      "  File \u001b[0;32m\"C:\\Users\\User\\anaconda\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow_datasets\\core\\dataset_builder.py\"\u001b[0m, line \u001b[0;32m188\u001b[0m, in \u001b[0;35m__init__\u001b[0m\n    self.info.initialize_from_bucket()\n",
      "  File \u001b[0;32m\"C:\\Users\\User\\anaconda\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow_datasets\\core\\dataset_info.py\"\u001b[0m, line \u001b[0;32m407\u001b[0m, in \u001b[0;35minitialize_from_bucket\u001b[0m\n    data_files = gcs_utils.gcs_dataset_info_files(self.full_name)\n",
      "  File \u001b[0;32m\"C:\\Users\\User\\anaconda\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow_datasets\\core\\utils\\gcs_utils.py\"\u001b[0m, line \u001b[0;32m64\u001b[0m, in \u001b[0;35mgcs_dataset_info_files\u001b[0m\n    filenames = [el for el in gcs_files(prefix_filter=prefix)\n",
      "  File \u001b[0;32m\"C:\\Users\\User\\anaconda\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow_datasets\\core\\utils\\gcs_utils.py\"\u001b[0m, line \u001b[0;32m55\u001b[0m, in \u001b[0;35mgcs_files\u001b[0m\n    xml_root = ElementTree.fromstring(top_level_xml_str)\n",
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\User\\anaconda\\anaconda3\\envs\\tensorflow_env\\lib\\xml\\etree\\ElementTree.py\"\u001b[1;36m, line \u001b[1;32m1315\u001b[1;36m, in \u001b[1;35mXML\u001b[1;36m\u001b[0m\n\u001b[1;33m    parser.feed(text)\u001b[0m\n",
      "\u001b[1;36m  File \u001b[1;32m\"<string>\"\u001b[1;36m, line \u001b[1;32munknown\u001b[0m\n\u001b[1;31mParseError\u001b[0m\u001b[1;31m:\u001b[0m mismatched tag: line 8, column 2\n"
     ]
    }
   ],
   "source": [
    "imdb, info = tfds.load('imdb_reviews/subwords8k', with_info= True, as_supervised= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
